\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry} % Adjust the margins
\usepackage{setspace} % For line spacing
\usepackage{titlesec} % For section formatting
\usepackage{graphicx}
\usepackage{float}
\usepackage{biblatex}
\usepackage{listings}
\usepackage{xcolor}
\renewcommand{\thefigure}{\arabic{figure}}


\lstset{
    basicstyle=\ttfamily\footnotesize,
    breaklines=true,
    frame=single,
    keywordstyle=\color{blue},
    commentstyle=\color{gray},
    stringstyle=\color{red},
    numbers=left,
    numberstyle=\tiny\color{gray},
    stepnumber=1,
    numbersep=5pt
}


% Set single-line spacing
\setstretch{1.0}

% Define section spacing
\titlespacing*{\section}{0pt}{1.5ex plus 1ex minus .2ex}{1ex plus .2ex}
\titlespacing*{\subsection}{0pt}{1ex plus .5ex minus .2ex}{0.5ex plus .2ex}

% Title information
\title{PCA based construction of cryptocurrency index Process Report}
\author{Zeyan Huang}
\date{} 

\begin{document}

\maketitle

% Abstract Section
\begin{abstract}

\end{abstract}

\section{Introduction}
After reviewing the paper "Principal Component Analysis-Based Construction and Evaluation of a Cryptocurrency Index"\cite{1}, I decided to replicate its results and explore potential improvements for analyzing the cryptocurrency market.

% Data Section
\section{Data}
To construct the dataset for this study, I collected daily market capitalization data for the top 1,200 cryptocurrencies from CoinGecko.com, covering the period from March 29, 2024, to March 28, 2025. The selection of these cryptocurrencies was based on their rankings as of March 28, 2025, ensuring that the dataset reflects the market structure as observed at the end of the analysis period.

One notable issue with the dataset is the presence of missing data points. Specifically, some cryptocurrencies had not yet launched at the start of the data period, resulting in incomplete time series that could complicate further analysis.

To address this issue, I adopted different approaches for handling missing data when calculating the static index and the dynamic index.


\section{Static Index}
The methodology presented in the paper uses the first principal component (PC1) as a set of weights, which are multiplied by the market capitalizations of the corresponding cryptocurrencies on a given day. The weighted values are then summed to compute the index value for each specific day.

In this replication, I applied principal component analysis to the full dataset---consisting of 1,200 cryptocurrencies over 365 days---and used PC1 as the weighting scheme to construct the daily index. However, the variance explained by PC1 was only 36\%, which is relatively low and suggests that the resulting index may not be as reliable or informative as implied in the original paper.
Here I used the sum of the 1200 cryptocurrencies' market capitalization as the total market capitalization.


\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{results/figures/static_index_pc1.png}
    \caption{Static Index using PC1}
    \label{fig:si1}
\end{figure}

As shown in the plot above, the calculated static index does not fully capture the overall movement of the cryptocurrency market. However, there are several periods where the index exhibits similar short-term trends to the total market capitalization, suggesting that the method may still partially reflect market dynamics.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{results/figures/static_index_pc2.png}
    \caption{Static Index using absolute value of PC1 and PC2}
    \label{fig:si1}
\end{figure}
Interestingly, when using the absolute value of the sum of PC1 and PC2 loadings as weights, the resulting static index appears to effectively capture the overall movement of the cryptocurrency market, even though the combined variance explained is only 61.5\%.

The static index without using absolute value provided below.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{results/figures/static_index_pc2_notabsolute.png}
    \caption{Static Index using PC2}
    \label{fig:si1}
\end{figure}

% Experiments Section
\section{Dynamic Index Construction Methodology}

Based on the PCA-based dynamic index methodology presented in the original paper, the calculation of the dynamic index consists of two main steps.

In the \textbf{first step}, a rolling window of \( T_n \) days (90 days in our implementation) is used to conduct PCA and correlation analysis to determine the number of constituent cryptocurrencies to include in the index. We define an update interval of \( T_{\text{n\_gap}} \) days (30 days in our test). Every 30 days, we extract the preceding 90-day window of data, apply PCA, and identify the subset of cryptocurrencies whose individual indices (constructed using PC1) have a correlation higher than 0.99 with the index derived from the full set of cryptocurrencies. This subset of cryptocurrencies is saved for use in the next step, and the number of selected assets is denoted as \( nc \).

In the \textbf{second step}, the index is computed using a forward-looking window. Specifically, for each update period, we take the previous \( T_w \) days (also 30 in our implementation) of data for the \( n_c \) selected cryptocurrencies and apply PCA. The first principal component (PC1) is used as the weighting vector. The index value \( a_t \) for the subsequent \( T_w \) days is calculated as:

\[
a_t = \sum_{i=1}^{n_c} \text{PC}_{1,i} \cdot M_{t,i}
\]

where \( \text{PC}_{1,i} \) is the loading of the \( i \)-th asset on the first principal component, and \( M_{t,i} \) is the market capitalization of asset \( i \) on day \( t \).

Once the daily values of \( a_t \) are obtained, the dynamic PCA-based index \( I_{\text{PCA}} \) is computed as:

\[
I_{\text{PCA}} = \frac{a_t}{a_{\text{base}}} \times m
\]

Here, \( m \) is a multiplier (set to 1000 for this test), and \( a_{\text{base}} \) is the value of \( a_t \) on the base day \( t_0 \), which is the first day of each new \( T_w \)-day sub-period. This ensures that the index resets to \( m \) at the beginning of each sub-period, maintaining consistency with the methodology described in the paper.

The plot below compares the dynamic PCA-based index with the total market capitalization, using the first principal component (PC1) as the weighting vector. While the dynamic index generally tracks the overall movement of the market quite well, a noticeable divergence appears after December, where the index underperforms relative to the market capitalization. I believed this gap is because our $m$ value is set as 1000 and not updated follows the market, so the next step should focus on the calculation methodology of choosing $m$.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{results/figures/dynamic_index_pc1.png}
    \caption{Dynamic index using PC1}
    \label{fig:di1}
\end{figure}

\

\subsection{Pursuing the method of updating the value of m}

A critical limitation identified in the initial dynamic index calculation was the divergence between the index and the total market capitalization over time. This was attributed to the static re-basing of the index value. The original methodology reset the index to a fixed value at the beginning of each new 30-day calculation window, which caused a "drift" and failed to preserve the continuity of the index's performance across periods. To address this, two alternative methods for updating the chain-linking base value, $m$, were investigated.

The first method attempted to smooth the transition between periods by setting the starting value of $m$ for a new calculation window to be the **mean of the index values from the previous 30-day window**. The hypothesis was that an average value would provide a more stable base. However, as shown in Figure \ref{fig:m_mean_30}, this approach resulted in significant deviations and did not consistently track the market capitalization, suggesting that the smoothed value could not keep pace with rapid market movements.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{results/figures/D_PCA_index_m_pre30.png} % User's first image
    \caption{Dynamic Index using m = Previous 30 Days' Data}
    \label{fig:m_mean_30}
\end{figure}

The second method implemented a more responsive, **day-by-day update rule**. Here, the value of $m$ for each new 30-day calculation window was set to the index value from the **immediately preceding day**. This ensures a much smoother and more accurate transition between calculation periods. The result of this methodology is displayed in Figure \ref{fig:m_prev_day}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{results/figures/D_PCA_index_m_pre.png} % User's second image
    \caption{Dynamic Index using m = Previous Day's Data}
    \label{fig:m_prev_day}
\end{figure}

As illustrated in Figure \ref{fig:m_prev_day}, this continuous, daily chain-linking method produced a dynamic index that tracks the total market capitalization with remarkable accuracy. The divergence issue is resolved, and the index now serves as a robust representation of the overall market's behavior.

Based on this comparative analysis, the decision was made to adopt the daily updating methodology (m = Previous day's data) for the final construction of the dynamic PCA-based index used in all subsequent factor analyses. This concludes the index construction phase of the project.

% Replace everything from here onwards in your .tex file

\section{Analysis and Interpretation of the Index}
To evaluate the explanatory power of the constructed dynamic PCA index, a multi-stage analysis was conducted. This involved assessing the index's sensitivity to common financial risk factors, exploring its predictability with advanced machine learning models, and forecasting its future path using time-series analysis.

\subsection{Three-Factor Model Framework}
The analysis begins with the application of a factor model, a standard and powerful technique in financial econometrics for decomposing portfolio returns and understanding risk exposures. These models explain the returns of a portfolio through its sensitivity (beta) to a set of common risk factors.

The pioneering work in this area is the Fama-French three-factor model. While the original Capital Asset Pricing Model (CAPM) proposed that market risk was the only systematic factor, Fama and French demonstrated that two additional factors—a size premium (SMB) and a value premium (HML)—significantly improve the explanatory power for stock market returns.

The goal of applying this framework to the PCA-based cryptocurrency index is twofold. First, it serves as a diagnostic tool to understand the economic risks to which the index is exposed. Second, it allows for a test of the index's "alpha," to see if the PCA construction methodology provides excess returns not explained by common risk factors. Adapting this model to the cryptocurrency market requires creating proxies for these factors, particularly the value factor, given the absence of traditional accounting metrics.

The formal regression model to be tested is:
\[
R_{i,t} - R_{f,t} = \alpha_i + \beta_{i,mkt} \cdot MKT_t + \beta_{i,smb} \cdot SMB_t + \beta_{i,hml} \cdot HML_t + \epsilon_{i,t}
\]
where $R_{i,t} - R_{f,t}$ is the excess return of the PCA index, $\alpha_i$ is the abnormal return, and $\beta$ values are the factor loadings.

\subsection{Factor Construction Methodology}
The analysis was built upon a unified dataset from CoinGecko, containing daily market capitalization and price data for the top 1,200 cryptocurrencies. This allowed for the daily construction of three factors.

\begin{itemize}
    \item \textbf{MKT (Market Factor):} Defined as the equal-weighted average return of all available cryptocurrencies minus the daily risk-free rate (3-Month Treasury Bill).
    \item \textbf{SMB (Small Minus Big) Factor:} The daily return of a portfolio of the 100 smallest market-cap cryptocurrencies minus the daily return of a portfolio of the 100 largest.
    \item \textbf{HML (High Minus Low) Value Factor:} Constructed using a short-term reversal strategy. Portfolios of recent "losers" (Value) and "winners" (Growth) were formed monthly based on the previous month's returns. HML is the daily return of the Value portfolio minus the Growth portfolio.
\end{itemize}

\subsection{Model Application and Results}
The analysis was conducted in three parts, progressively increasing in complexity to build a comprehensive understanding of the index. All models were evaluated using `TimeSeriesSplit` cross-validation to prevent look-ahead bias.

\subsubsection{Part 1: Analysis with Original 3 Factors}
In the first stage, five models were trained using only MKT, SMB, and WML as predictors. Table \ref{tab:3factor_coeffs} summarizes the coefficients and statistical significance from the linear models.

\begin{table}[H]
    \centering
    \caption{Linear Model Coefficients on Original 3 Factors}
    \label{tab:3factor_coeffs}
    \begin{tabular}{lrrrr}
        \hline
        \textbf{Factor} & \textbf{OLS Coefficient} & \textbf{P-Value} & \textbf{Ridge Coeff (Scaled)} & \textbf{Lasso Coeff (Scaled)} \\
        \hline
        Intercept & 0.001734 & 0.0479 & - & - \\
        MKT & 0.699650 & 0.0000 & 0.022506 & 0.021646 \\
        SMB & -0.005889 & 0.8639 & -0.000252 & -0.0 \\
        WML & 0.009064 & 0.8285 & 0.000611 & 0.0 \\
        \hline
    \end{tabular}
\end{table}

The results clearly show that the MKT factor is highly statistically significant ($p < 0.001$), while SMB and WML are not. The Lasso regression confirms this by shrinking the coefficients for SMB and WML to exactly zero. The model performance is summarized in Table \ref{tab:3factor_perf}.

\begin{table}[H]
    \centering
    \caption{Model Performance on Original 3 Factors}
    \label{tab:3factor_perf}
    \begin{tabular}{lr}
        \hline
        \textbf{Model} & \textbf{Out-of-Sample R-squared} \\
        \hline
        OLS & 0.738155 \\
        Ridge & 0.740393 \\
        Lasso & 0.736818 \\
        Random Forest & 0.613215 \\
        XGBoost & 0.544672 \\
        \hline
    \end{tabular}
\end{table}

\subsubsection{Part 2: Analysis with Engineered Features}
In the second stage, the feature set was expanded with time-series lags and rolling statistics to determine if more complex, time-dependent patterns could improve the model's explanatory power. All five models were retrained on this expanded feature set.

The OLS regression on the richer feature set (Table \ref{tab:eng_coeffs}) showed that while the MKT factor remained the most statistically significant predictor, the Lasso model was highly selective, shrinking almost all other new coefficients to zero, leaving only MKT and a small, negative coefficient for `WML\_lag\_1`.

\begin{table}[H]
    \centering
    \caption{OLS, Ridge, and Lasso Coefficients on Engineered Features}
    \label{tab:eng_coeffs_full}
    \resizebox{\textwidth}{!}{% Scale table to fit page width
    \begin{tabular}{lrrrr}
        \hline
        \textbf{Feature} & \textbf{OLS Coefficient} & \textbf{P-Value} & \textbf{Ridge Coeff (Scaled)} & \textbf{Lasso Coeff (Scaled)} \\
        \hline
        WML\_roll\_avg\_30D & 0.747198 & 1.008787e-01 & 0.001127 & 0.0 \\
        MKT & 0.716907 & 7.729564e-41 & 0.018497 & 0.021632 \\
        MKT\_roll\_avg\_30D & -0.511502 & 8.873644e-02 & -0.000930 & 0.0 \\
        SMB\_roll\_avg\_7D & -0.355890 & 2.223056e-01 & -0.001307 & -0.0 \\
        WML\_roll\_std\_7D & -0.315234 & 9.704901e-02 & -0.000940 & 0.0 \\
        SMB\_roll\_avg\_30D & 0.211461 & 3.789729e-01 & 0.000786 & 0.0 \\
        MKT\_roll\_std\_7D & 0.186708 & 1.360852e-01 & 0.000847 & 0.0 \\
        MKT\_roll\_std\_30D & -0.119151 & 7.207842e-01 & -0.000249 & -0.0 \\
        WML\_roll\_std\_30D & 0.103962 & 6.999439e-01 & -0.000345 & 0.0 \\
        SMB\_roll\_std\_30D & -0.092348 & 7.754201e-01 & -0.000884 & -0.0 \\
        WML\_lag\_1 & -0.075972 & 2.542202e-01 & -0.001542 & -0.000194 \\
        \hline
    \end{tabular}
    }
\end{table}

The overall performance results are summarized in Table \ref{tab:eng_perf}.

\begin{table}[H]
    \centering
    \caption{Model Performance on Engineered Features}
    \label{tab:eng_perf}
    \begin{tabular}{lr}
        \hline
        \textbf{Model} & \textbf{Out-of-Sample R-squared} \\
        \hline
        OLS & 0.735528 \\
        Ridge & 0.743995 \\
        Lasso & 0.734962 \\
        Random Forest & 0.603221 \\
        XGBoost & 0.575352 \\
        \hline
    \end{tabular}
\end{table}

Contrary to what might be expected, the addition of engineered features did not improve, and in some cases worsened, the out-of-sample performance of the models. The **Ridge regression model remained the top performer**, achieving an R-squared of approximately 0.744. The complex, tree-based models (Random Forest and XGBoost) performed significantly worse than the linear models.

This suggests that the underlying relationship between the factors and the index's returns is fundamentally linear. The new features, while potentially capturing some market dynamics, likely introduced more noise than signal, which the more flexible tree-based models overfitted to during training. The simpler linear models, particularly Ridge with its regularization, were more robust to this noise and provided a more stable and generalizable explanation.

Even though the XGBoost model was not the best performer, its feature importance plot (Figure \ref{fig:feature_importance}) is still insightful, as it shows which variables the non-linear model attempted to prioritize. It confirmed the dominance of the MKT factor, but also highlighted that the model was paying attention to features like the volatility of the SMB factor and lags of the MKT factor.

\begin{figure}[H]
    \centering
    % You would place your feature importance plot image file here
    \includegraphics[width=0.8\textwidth]{results/figures/ml_model_feature_importance.png}
    \caption{Top 15 Feature Importances from XGBoost Model}
    \label{fig:feature_importance}
\end{figure}

\subsubsection{Part 3: Univariate Forecasting with ARIMA}
The final stage of analysis employed an AutoRegressive Integrated Moving Average with eXogenous variables (ARIMAX) model to forecast the index value itself. This approach models a series based on its own past values while incorporating the predictive power of external factors.

An ARIMAX(1,1,1) model was selected based on stationarity tests and analysis of ACF/PACF plots (Figure \ref{fig:acf_pacf}).

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{results/figures/ARIMA_autocorrelation.png}
    \caption{ACF and PACF plots for the differenced time series.}
    \label{fig:acf_pacf}
\end{figure}

The model was fitted on a training set, holding out the final 30 days of data for validation. The model summary reinforced the previous findings: the MKT factor is highly significant, while other factors and the internal AR/MA terms are not. Finally, the model's out-of-sample forecast was compared against the actual held-out data, as shown in Figure \ref{fig:arima_forecast}.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{results/figures/ARIMA_output_vs.png}
    \caption{ARIMAX Out-of-Sample Forecast vs. Actual Data}
    \label{fig:arima_forecast}
\end{figure}

As the plot illustrates, the ARIMAX model successfully captured the general downward trend of the index during the out-of-sample test period. A key strength of the model is that the actual index values remained well within the 95\% confidence interval for the entire forecast horizon, indicating that the model provided a reliable and well-calibrated range of expected outcomes. As is typical for statistical forecasting models, it did not predict the sharp, short-term volatility but rather provided a smoothed-out expectation of the index's future path. This demonstrates its utility as a tool for trend forecasting rather than high-frequency price prediction.

\subsection{Overall Conclusion}
This project successfully constructed a dynamic, PCA-based cryptocurrency index and subjected it to a rigorous, multi-stage analytical evaluation to understand its underlying drivers. The comprehensive modeling process, spanning from traditional factor models to modern machine learning techniques and time-series forecasting, yields a clear and consistent primary conclusion.

The factor analysis demonstrates that the PCA-based dynamic index is, first and foremost, a market-tracking vehicle. Its returns are strongly and significantly explained by its exposure to the overall cryptocurrency market (MKT), exhibiting a stable market beta of approximately 0.70. This core relationship proved to be the most dominant feature across all models. In contrast, the analysis revealed that the index has no statistically significant tilt towards the traditional size (SMB) or value/momentum (WML) factors in their basic forms. This conclusion was robustly supported by both the high p-values in the OLS regression and by the Lasso model's aggressive feature selection, which consistently eliminated these factors.

A key shortcoming of this project, however, lies in the construction of the factors themselves. The attempt to enhance predictive power with more complex, engineered features did not meaningfully improve the results, with the simple linear models remaining superior. I believe this is a direct result of the limitations in the available data. Creating truly robust factors requires more than just price and market cap history. For instance, a proper HML (Value) factor would ideally use on-chain metrics like the NVT ratio, but sourcing this kind of granular data for over 1,000 assets is a significant challenge without the institutional resources of a large financial firm. The difficulty in constructing truly independent, powerful risk factors for the crypto space was a major hurdle in this research.

Finally, the ARIMAX forecasting model further solidified these findings. It confirmed that the MKT factor's predictive power subsumes the information contained within the index's own time-series patterns. While the model successfully forecasted the general out-of-sample trend, its performance was driven by the exogenous market factor rather than its autoregressive properties.

In summary, the PCA methodology successfully creates a diversified index that effectively tracks the market. The project clearly shows that this index's risk profile is best explained by a simple, single-factor model based on the market itself. While deeper analysis hints at secondary, non-linear dynamics related to volatility and market memory, fully exploring them would require overcoming the significant data-sourcing and financial challenges inherent in advanced quantitative research, defining a clear path for future work should such resources become available.

\end{document}


% Reference
\newpage
\begin{thebibliography}{1}
\bibitem{1} Principal component analysis based construction and evaluation of cryptocurrency index. Retrieved from \url{https://doi.org/10.1016/j.eswa.2020.113796}
\end{thebibliography}


% Appendix Section
\newpage
\appendix
\section*{Appendix}
\subsection*{R Code for Data Preprocessing}
\begin{lstlisting}[language=R, caption={Data Preprocessing}]
# Data
# Data preparation
# data from Coingecko
folder_path <- "market_cap_data/"
all_files <- list.files(folder_path)


dataset <- lapply(all_files, function(x){
  data <- read.csv(paste0(folder_path, x), header = TRUE)
  data <- data[-nrow(data), ]
  data$Date <- as.Date(data$date)
  data <- data[order(data$Date),]
  data <- data[!duplicated(data$Date), ]

  
  price <- as.numeric(data$market_cap)
  names(price) <- data$Date
  return(price)
}
  )

# Name the data list
file_names <- gsub("\\.csv", "", all_files)
names(dataset) <- file_names

# Step 1: Convert each price series to a dataframe with Date + market cap
dataset_aligned <- lapply(names(dataset), function(name) {
  df <- data.frame(Date = as.Date(names(dataset[[name]])),
                   Cap = dataset[[name]])
  colnames(df)[2] <- name
  return(df)
})

# Step 2: Merge all dataframes by Date, using full outer join
data <- Reduce(function(x, y) merge(x, y, by = "Date", all = TRUE),
                      dataset_aligned)

# Step 3: set all NA value as 0
# data[is.na(data)] <- 0

# Set Date as row index
rownames(data) <- data$Date
data$Date <- NULL
\end{lstlisting}

\subsection*{R Code for Static Index}
\begin{lstlisting}[language=R, caption={SI}]
# Static Index
# Data prep
data_static_index <- data

data_static_index[is.na(data_static_index)] <- 0

(pca_static_index$sdev[1:7])^2
sum((pca_static_index$sdev)^2)
sum((pca_static_index$sdev[1:2])^2)/sum((pca_static_index$sdev)^2)*100

static_index <- as.matrix(data_static_index) %*% as.vector(pca_static_index$rotation[,1])

static_index <- as.data.frame(static_index)

static_index$Date <- as.Date(rownames(static_index))

plot(static_index$Date, static_index$V1, type = "l", xlab = "Date", ylab = "PCA Static Index")

# Get the daily market capitalization
data_mc <- data.frame(RowSum = rowSums(data_static_index))

rownames(data_mc) <- rownames(data_static_index)

# Combine the data_mc with static_index
data_mc_si <- merge(static_index, data_mc, by = "row.names")
rownames(data_mc_si) <- data_mc_si$Row.names
data_mc_si$Row.names <- NULL

(data_mc_si$Date, data_mc_si$V1, type = "l", col = "blue", xlab = "Date", ylab = "Static Index")

par(new = TRUE)
plot(data_mc_si$Date, data_mc_si$RowSum, type = "l", col = "red", axes = FALSE, xlab = "", ylab = "")
axis(side = 4)  # Add right-side axis
mtext("Market Capitalization", side = 4, line = 3)  # Label for right Y-axis

# Add legend
legend("topright", legend = c("Static Index", "Market Capitalization"),
       col = c("blue", "red"), lty = 1, cex = 0.5)

static_index_pc2 <- abs(as.matrix(data_static_index) \%*\% as.vector(rowSums(pca_static_index$rotation[,1:2])))

static_index_pc2  <- as.data.frame(static_index_pc2 )

static_index_pc2$Date <- as.Date(rownames(static_index_pc2))

# Combine the data_mc with static_index
data_mc_si_pc2 <- merge(static_index_pc2, data_mc, by = "row.names")
rownames(data_mc_si_pc8) <- data_mc_si_pc2$Row.names
data_mc_si_pc2$Row.names <- NULL

plot(data_mc_si_pc2$Date, data_mc_si_pc2$V1, type = "l", col = "blue", xlab = "Date", ylab = "Static Index")

par(new = TRUE)
plot(data_mc_si_pc2$Date, data_mc_si_pc2$RowSum, type = "l", col = "red", axes = FALSE, xlab = "", ylab = "")
axis(side = 4)  # Add right-side axis
mtext("Market Capitalization", side = 4, line = 3)  # Label for right Y-axis

# Add legend
legend("topright", legend = c("Static Index", "Market Capitalization"),
       col = c("blue", "red"), lty = 1, cex = 0.5)

\end{lstlisting}


\subsection*{R Code for Nc constituents Update function}
\begin{lstlisting}[language=R, caption={NC}]
Nc_cal <- function(dataframe, threshold = 0.99){
  # Step 1: Calculate PC1 for all columns
  pca_full_temp <- prcomp(dataframe, scale. = TRUE)

  # Initialize a list to store PC1 for each subset of columns
  pc1_list_temp <- list()
  index_list_temp <- list()
  variance_explained_list <- list()

  # Loop through subsets of columns (from 1:2, 1:3, ..., 1:ncol)
  for (i in 2:ncol(dataframe)) {
    subset_data <- dataframe[, 1:i]
    pca_subset <- prcomp(subset_data, scale. = TRUE)
    pc1_subset <- pca_subset$rotation[, 1]
    
    pc_1_v <- (pca_subset$sdev[1])^2
    pc_all_v <- sum((pca_subset$sdev)^2)
    pc_1_explained_v_pct <- pc_1_v/pc_all_v
    variance_explained_list[[i]] <- list(pc_1_v, pc_all_v, pc_1_explained_v_pct)
    
    temp_index <- as.matrix(subset_data) %*% as.vector(pc1_subset)
    pc1_list_temp[[i]] <- pc1_subset  # Store in list (index starts at 1)
    index_list_temp[[i]] <- temp_index
  }

  # Initialize a vector to store the correlations
  correlations_temp <- numeric(length(pc1_list_temp) - 1)

  # Index with all crypto
  index_all_temp <- index_list_temp[[length(index_list_temp)]]

  # Calculate the Correlation
  for (i in 2: (length(index_list_temp))){
    correlations_temp[i] <- cor(index_list_temp[[i]], index_all_temp)
  }

  # Find the first index where correlation >= threshold
  first_reach_index_temp <- which(correlations_temp >= threshold)[1]
  chosen_crypto_list <- c(colnames(dataframe)[1:first_reach_index_temp])

  output_list <- list()
  output_list[['Index']] <- index_list_temp
  output_list[['Correlation']] <- correlations_temp
  output_list[['Nc']] <- first_reach_index_temp
  output_list[['Choosen Crypto']] <- chosen_crypto_list
  output_list[['Variance']] <- variance_explained_list
  return(output_list)
}
\end{lstlisting}

\subsection*{R Code for Basic dynamic index calculation function for single period}
\begin{lstlisting}[language=R, caption={DI_base}]
dynamic_index_base <- function(dataframe, Tw = 30){
  a_list <- c()
  a_base <- as.numeric(dataframe[Tw+1,])%*%as.vector(prcomp(dataframe[1:Tw,], scale.= TRUE)$rotation[,1])
  a_list <- append(a_list, 1)
  # for (i in 2:Tw){
  #   a_t <- (as.numeric(dataframe[Tw+i,])%*%as.vector(prcomp(dataframe[i:(Tw+i-1),], scale.= TRUE)$rotation[,1]))/a_base
  #   a_list <- c(a_list, a_t)
  #   
  # }
  # a_t_df <- data.frame(Value = a_list, row.names = rownames(dataframe[(Tw+1):(Tw+Tw),]))
  a_t <- as.matrix(dataframe[(Tw+2):(Tw+Tw),])%*%as.vector(prcomp(dataframe[1:Tw,], scale.= TRUE)$rotation[,1])
  
  a_list <- c(a_list, as.vector(a_t)/c(a_base))
  a_t_df <- data.frame(Value = a_list, row.names = rownames(dataframe[(Tw+1):(Tw+Tw),]))
  
  return(a_t_df)
}
\end{lstlisting}

\subsection*{R Code for Dynamic Index function}
\begin{lstlisting}[language=R, caption={DI_{base}}]
dynamic_index_cal <- function(dataframe, Tn = 90, Tn_gap = 30, Tw = 30, threshold = 0.99){
  
  # Nc Choosing process
  nc_result <- list()
  n_iter <- floor((nrow(dataframe) - Tn) / Tn_gap)
  for (i in 0:n_iter){
    data_90 <- dataframe %>%
    slice((1+i*30):(90+i*30)) %>%       # Select the first 90 rows (time window)
    select(where(~ all(!is.na(.)))) %>% # Drop columns that contain any NA values
    select(where(~ all(. != 0))) %>%    # Drop columns that contain any 0 values
    select(where(~ {                    # Drop columns with 0 or NA standard deviation
      sd_x <- sd(., na.rm = TRUE)       # Calculate standard deviation ignoring NA
      is.finite(sd_x) && sd_x > 0       # Keep only columns with valid, non-zero SD
    })) %>%
    { .[, order(-as.numeric(.[1, ]))] } # Sort the remaining columns by the first row, descending
    
    nc_result[[i+1]] <- Nc_cal(data_90, threshold = threshold)
  }
  
  # Dynamic Index Calculation
  dynamic_index_results <- list()
  for (i in 1:length(nc_result)){
    data_60 <- dataframe %>%
    slice((31+i*30):(90+i*30)) %>%       # Select the required 60 rows (time window)
    select(where(~ all(!is.na(.)))) %>% # Drop columns that contain any NA values
    select(where(~ all(. != 0))) %>%
    select(all_of(nc_result[[i]][["Choosen Crypto"]]))# Filter by selected crypto
    if (nrow(data_60) < 60){
      break
    }
    dynamic_index_results[[i]] <- dynamic_index_base(data_60)
  }
  
  final_results <- list(nc_result, dynamic_index_results)
  return(final_results)
}
\end{lstlisting}

\end{document}
